---
title: 虚拟网卡 tun/tap - 阅读转载
date: 2023-09-11 21:57:37
categories:
 - other
tags:
 - networking
typora-root-url: ../../../static
---

原文: 

- [云原生虚拟网络 tun/tap & veth-pair - luozhiyun`s Blog](https://www.luozhiyun.com/archives/684)
- [理解 Linux 虚拟网卡设备 tun/tap 的一切 | 骏马金龙](https://www.junmajinlong.com/virtual/network/all_about_tun_tap/)

之前的相关文章:

- [TUN Device & utun Interface - David's Blog](https://davidzhu.xyz/post/cs-basics/011-tun-device/)
- [Tunneling Protocols - David's Blog](https://davidzhu.xyz/post/cs-basics/013-tunneling-protocols/)

## 1. 物理网卡和虚拟网卡

### 1.1. 物理网卡收发数据的流程

物理网卡可以接收和发送数据：

- 收：外界向该物理网卡发送数据时，外界发送到网卡的数据最终会传输到内核空间的网络协议栈中
- 发：本机要从物理网卡发送数据时，数据将从内核的网络协议栈传输到网卡，网卡负责将数据发送出去
- 现在的网卡具备 DMA 能力，所以网卡和网络协议栈之间的数据传输由网卡负责，而非由内核亲自占用 CPU 来执行读和写

一般来说，数据的起点和终点是用户程序，所以多数时候的数据需要在用户空间和内核空间 (网络协议栈) 再传输一次：

- 当用户进程的数据要发送出去时，数据从用户空间写入内核的网络协议栈，再从网络协议栈传输到网卡，最后发送出去
- 当用户进程等待外界响应数据时，数据从网卡流入，传输至内核的网络协议栈，最后数据写入用户空间被用户进程读取

在这些过程中，**内核和用户空间的数据传输由内核占用 CPU 来完成，内核和网卡之间的数据传输由网卡的 DMA 来完成**，不需要占用过多的 CPU。

<img src="/007-tun-tap-veth-reading/f.png" alt="f" style="zoom: 50%;" />

### 1.2. 虚拟网卡和物理网卡的对比

和物理网卡对比一下，物理网卡是硬件网卡，它位于硬件层，虚拟网卡则可以看作是用户空间的网卡，就像用户空间的文件系统 (fuse) 一样。

物理网卡和虚拟网卡唯一的不同点在于物理网卡本身的硬件功能：物理网卡以比特流的方式传输数据。

也就是说，内核会公平对待物理网卡和虚拟网卡，物理网卡能做的配置，虚拟网卡也能做。比如可以为虚拟网卡接口配置 IP 地址、设置子网掩码，可以将虚拟网卡接入网桥等等。

只有在数据流经物理网卡和虚拟网卡的那一刻，才会体现出它们的不同，即传输数据的方式不同：物理网卡以比特流的方式传输数据，虚拟网卡则直接在内存中拷贝数据 (即，在内核之间和读写虚拟网卡的程序之间传输)。

正因为虚拟网卡不具备物理网卡以比特流方式传输数据的硬件功能，所以，**绝不可能通过虚拟网卡向外界发送数据，外界数据也不可能直接发送到虚拟网卡上**。能够直接收发外界数据的，只能是物理设备。

虽然虚拟网卡无法将数据传输到外界网络，但却：

- **可以将数据传输到本机的另一个网卡 (虚拟网卡或物理网卡) 或其它虚拟设备 (如虚拟交换机) 上**
- **可以在用户空间运行一个可读写虚拟网卡的程序，该程序可将流经虚拟网卡的数据包进行处理**，这个用户程序就像是物理网卡的硬件功能一样，可以收发数据 (可将物理网卡的硬件功能看作是嵌入在网卡上的程序)，比如 OpenVPN 就是这样的工具

很多人会误解这样的用户空间程序，认为它们可以对数据进行封装。比如认为 OpenVPN 可以在数据包的基础上再封装一层隧道 IP 首部，但这种理解是错的。

一定请注意，**用户空间的程序是无法对数据包做任何封装和解封操作的，所有的封装和解封都只能由内核的网络协议栈来完成**。

<img src="/007-tun-tap-veth-reading/f-4483381.png" alt="f" style="zoom:50%;" />

使用 OpenVPN 之所以可以对数据再封装一层隧道 IP 层，是因为 OpenVPN 可以读取已经封装过一次 IP 首部的数据，并将包含 ip 首部的数据作为普通数据通过虚拟网卡再次传输给内核。因为内核接收到的是来自虚拟网卡的数据，所以内核会将其当作普通数据从头开始封装 (从四层封装到二层封装)。当数据从网络协议栈流出时，就有了两层 IP 首部的封装。

换句话说，每一次看似由用户空间程序进行的额外封装，都意味着数据要从内核空间到用户空间，再到内核空间。以 OpenVPN 为例：

```
tcp/ip stack --> tun --> OpenVPN --> tcp/ip stack --> Phyical NIC
```

其中 tun 是 OpenVPN 创建的一个三层虚拟网卡，tun 设备在用户空间和内核空间之间传递数据。

具体的 openvpn 数据封装和数据流向的细节，参考更详细的[通过 openvpn 分析 tun 实现隧道的数据流程](https://www.junmajinlong.com/virtual/network/data_flow_about_openvpn)。

### 1.3. 程序写入虚拟网卡时的注意事项

用户空间的程序不可随意向虚拟网卡写入数据，因为写入虚拟网卡的这些数据都会被内核网络协议栈进行解封处理，就像来自物理网卡的数据都会被解封一样。

因此，如果**用户空间程序要写 tun/tap 设备，所写入的数据需具有特殊结构**：

- 要么是已经封装了 PORT 的数据，即传输层的 tcp 数据段或 udp 数据报
- 要么是已经封装了 IP+PORT 的数据，即 ip 层数据包
- 要么是已经封装了 IP+PORT+MAC 的数据，即链路层数据帧
- 要么是其它符合 tcp/ip 协议栈的数据，比如二层的 PPP 点对点数据，比如三层的 icmp 协议数据

也就是说，程序只能向虚拟网卡写入已经封装过的数据。

由于网络数据的封装都由内核的网络协议栈负责，所以程序写入虚拟网卡的数据实际上都原封不动地来自于上一轮的网络协议栈，用户空间程序无法对这部分数据做任何修改。

也就是说，这时**写虚拟网卡的用户空间程序仅充当了一个特殊的【转发】程序：要么转发四层 tcp/udp 数据，要么转发三层数据包，要么转发二层数据帧**。

这一段话可能不好理解，下面给个简单的示例分析。

假如物理网卡 eth0 从外界网络接收了这么一段特殊的 ping 请求数据：

<img src="/007-tun-tap-veth-reading/g.png" alt="g" style="zoom:50%;" />

这份数据会从物理网卡传输到内核网络协议栈，网络协议栈会对其解封，解封的内容只能是 tcp/ip 协议栈中的内容，即只能解封帧头部、IP 头部以及端口头部，网络协议栈解封后还剩下一段包含了内层 IP 头部 (tun 的 IP) 以及 icmp 请求的数据。

内核会根据刚才解封的端口号找到对应的服务进程，并将解封剩下的数据传输给该进程，即传输给用户空间的程序。

用户空间的程序不做任何修改地将读取到的包含了内层 IP 头部和 ICMP 请求的数据原封不动地写入虚拟网卡设备，内核从虚拟网卡接收到数据后，将数据进行解封，解封得到最终的 icmp 请求数据，于是内核开始构建用于响应 ping 请求的数据。

## 2. 概述

目前主流的虚拟网卡方案有[tun/tap](https://www.kernel.org/doc/Documentation/networking/tuntap.txt)和[veth](https://man7.org/linux/man-pages/man4/veth.4.html)两种。在时间上 tun/tap 出现得更早，在 Linux Kernel 2.4 版之后发布的内核都会默认编译 tun/tap 的驱动。并且 tun/tap 应用非常广泛，其中云原生虚拟网络中， flannel 的 UDP 模式中的 flannel0 就是一个 tun 设备，OpenVPN 也利用到了 tun/tap 进行数据的转发。

veth 是另一种主流的虚拟网卡方案，在 Linux Kernel 2.6 版本，Linux 开始支持网络名空间隔离的同时，也提供了专门的虚拟以太网（Virtual Ethernet，习惯简写做 veth）让两个隔离的网络名称空间之间可以互相通信。veth 实际上不是一个设备，而是一对设备，因而也常被称作 Veth-Pair。

Docker 中的 Bridge 模式就是依靠 veth-pair 连接到 docker0 网桥上与宿主机乃至外界的其他机器通信的。

<img src="/007-tun-tap-veth-reading/a.png" alt="a" style="zoom:50%;" />

## 3. tun/tap

tun 和 tap 是两个相对独立的虚拟网络设备，它们作为虚拟网卡，除了不具备物理网卡的硬件功能外，它们和物理网卡的功能是一样的，此外tun/tap负责在内核网络协议栈和用户空间之间传输数据。

- tun 设备是一个三层网络层设备，从 /dev/net/tun 字符设备上读取的是 IP 数据包，写入的也只能是 IP 数据包，因此常用于一些点对点IP隧道，例如OpenVPN，IPSec等；
- tap 设备是二层链路层设备，等同于一个以太网设备，从 /dev/tap0 字符设备上读取 MAC 层数据帧，写入的也只能是 MAC 层数据帧，因此常用来作为虚拟机模拟网卡使用；

<img src="/007-tun-tap-veth-reading/b.png" alt="b" style="zoom:50%;" />

从上面图中，我们可以看出物理网卡和 tun/tap 设备模拟的虚拟网卡的区别，虽然它们的一端都是连着网络协议栈，但是物理网卡另一端连接的是物理网络，而 tun/tap 设备另一端连接的是一个文件作为传输通道。

根据前面的介绍，我们大约知道虚拟网卡主要有两个功能，一个是连接其它设备（虚拟网卡或物理网卡）和 Bridge 这是 tap 设备的作用；另一个是提供用户空间程序去收发虚拟网卡上的数据，这是 tun 设备的作用。

主要区别是因为它们作用在不同的网络协议层，换句话说 tap设备是一个二层设备所以通常接入到 Bridge上作为局域网的一个节点，tun设备是一个三层设备通常用来实现 vpn。

## 4. OpenVPN 使用 tun 设备收发数据

OpenVPN 是使用 tun 设备的常见例子，它可以方便的在不同网络访问场所之间搭建类似于局域网的专用网络通道。其核心机制就是在 OpenVPN 服务器和客户端所在的计算机上都安装一个 tun 设备，通过其虚拟 IP 实现相互访问。

例如公网上的两个主机节点A、B，物理网卡上配置的IP分别是 ipA_eth0 和 ipB_eth0。然后在A、B两个节点上分别运行 openvpn 的客户端和服务端，它们会在自己的节点上创建 tun 设备，且都会读取或写入这个 tun 设备。

假设这两个设备对应的虚拟 IP 是 ipA_tun0 和 ipB_tun0，那么节点 B 上面的应用程序想要通过虚拟 IP 对节点 A 通信，那么数据包流向就是：

<img src="/007-tun-tap-veth-reading/c.png" alt="c" style="zoom:50%;" />

用户进程对 ipA_tun0 发起请求，经过路由决策后内核将数据从网络协议栈写入 tun0 设备；然后 OpenVPN 从字符设备文件中读取 tun0 设备数据，将数据请求发出去；内核网络协议栈根据路由决策将数据从本机的 eth0 接口流出发往 ipA_eth0 。

同样我们来看看节点 A 是如何接受数据：

<img src="/007-tun-tap-veth-reading/d.png" alt="d" style="zoom:50%;" />

当节点A 通过物理网卡 eth0 接受到数据后会将写入内核网络协议栈，因为目标端口号是OpenVPN程序所监听的，所以网络协议栈会将数据交给 OpenVPN ；

OpenVPN 程序得到数据之后，发现目标IP是tun0设备的，于是将数据从用户空间写入到字符设备文件中，然后 tun0 设备会将数据写入到协议栈中，网络协议栈最终将数据转发给应用进程。

从上面我们知道使用 tun/tap 设备传输数据需要经过两次协议栈，不可避免地会有一定的性能损耗，如果条件允许，容器对容器的直接通信并不会把 tun/tap 作为首选方案，一般是基于稍后介绍的 veth 来实现的。但是 tun/tap 没有 veth 那样要求设备成对出现、数据要原样传输的限制，数据包到用户态程序后，程序员就有完全掌控的权力，要进行哪些修改，要发送到什么地方，都可以编写代码去实现，因此 tun/tap 方案比起 veth 方案有更广泛的适用范围。

## 5. flannel UDP 模式使用 tun 设备收发数据

早期 flannel 利用 tun 设备实现了 UDP 模式下的跨主网络相互访问，实际上原理和上面的 OpenVPN 是差不多的。

在 flannel 中 flannel0 是一个三层的 tun 设备，用作在操作系统内核和用户应用程序之间传递 IP 包。当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 flanneld 进程，flanneld 进程是一个 UDP 进程，负责处理 flannel0 发送过来的数据包：

<img src="/007-tun-tap-veth-reading/e.png" alt="e" style="zoom:50%;" />

flanneld 进程会根据目的 IP 的地址匹配到对应的子网，从 Etcd 中找到这个子网对应的宿主机 Node2 的 IP 地址，然后将这个数据包直接封装在 UDP 包里面，然后发送给 Node 2。由于每台宿主机上的 flanneld 都监听着一个 8285 端口，所以 Node2 机器上 flanneld 进程会从 8285 端口获取到传过来的数据，解析出封装在里面的发给 ContainerA 的 IP 地址。

flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。然后网络栈会将这个数据包根据路由发送到 docker0 网桥，docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 veth pair 设备进入到 containerA 的 Network Namespace 里。

上面所讲的 Flannel UDP 模式现在已经废弃，原因就是因为它经过三次用户态与内核态之间的数据拷贝。容器发送数据包经过 docker0 网桥进入内核态一次；数据包由 flannel0 设备进入到 flanneld 进程又一次；第三次是 flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。

## 6. 总结

本篇文章只是讲了两种常见的虚拟网络设备。起因是在看 flannel 的时候，书里面都会讲到 flannel0 是一个 tun 设备，但是什么是 tun 设备却不明白，所以导致 flannel 也看的不明白。

经过研究，发现 tun/tap 设备是一个虚拟网络设备，负责数据转发，但是它需要通过文件作为传输通道，这样不可避免的引申出 tun/tap 设备为什么要转发两次，这也是为什么 flannel 设备 UDP 模式下性能不好的原因，导致了后面这种模式被废弃掉。

因为 tun/tap 设备作为虚拟网络设备性能不好，容器对容器的直接通信并不会把 tun/tap 作为首选方案，一般是基于后面介绍的 veth 来实现的。veth 作为一个二层设备，可以让两个隔离的网络名称空间之间可以互相通信，不需要反复多次经过网络协议栈， veth pair 是一端连着协议栈，另一端彼此相连的，数据之间的传输变得十分简单，这也让 veth 比起 tap/tun 具有更好的性能。

了解更多:

- [Introduction to Linux interfaces for virtual networking | Red Hat Developer](https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking)
- [Linux Virtual Networking](https://gist.github.com/mtds/4c4925c2aa022130e4b7c538fdd5a89f)