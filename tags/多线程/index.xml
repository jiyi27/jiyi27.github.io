<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>多线程 on 为霜的博客</title>
    <link>https://blog.jiyi27.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/</link>
    <description>Recent content in 多线程 on 为霜的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 26 Aug 2023 19:03:59 +0000</lastBuildDate><atom:link href="https://blog.jiyi27.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Is Multithreaded Server Better than a Single Thread Server?</title>
      <link>https://blog.jiyi27.com/posts/cs-basics/007-multithread-singlethread-server/</link>
      <pubDate>Sat, 26 Aug 2023 19:03:59 +0000</pubDate>
      
      <guid>https://blog.jiyi27.com/posts/cs-basics/007-multithread-singlethread-server/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; &lt;a href=&#34;https://qr.ae/pyztor&#34;&gt;https://qr.ae/pyztor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Why is a multithreaded web server better than a single thread server?&lt;/em&gt; &lt;strong&gt;It isn’t.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are four basic ways how a web server can handle concurrency:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;forking an OS process per request (like old versions of Apache)&lt;/li&gt;
&lt;li&gt;spawning an OS thread per request (like a new versions of Apache)&lt;/li&gt;
&lt;li&gt;using a single-threaded event loop (like nginx)&lt;/li&gt;
&lt;li&gt;using green threads or lightweight processes scheduled by a VM runtime instead of the OS (like in Erlang)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Currently the most common approaches are number 2 and 3.&lt;/p&gt;</description>
      <content>&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt; &lt;a href=&#34;https://qr.ae/pyztor&#34;&gt;https://qr.ae/pyztor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Why is a multithreaded web server better than a single thread server?&lt;/em&gt; &lt;strong&gt;It isn’t.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are four basic ways how a web server can handle concurrency:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;forking an OS process per request (like old versions of Apache)&lt;/li&gt;
&lt;li&gt;spawning an OS thread per request (like a new versions of Apache)&lt;/li&gt;
&lt;li&gt;using a single-threaded event loop (like nginx)&lt;/li&gt;
&lt;li&gt;using green threads or lightweight processes scheduled by a VM runtime instead of the OS (like in Erlang)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Currently the most common approaches are number 2 and 3.&lt;/p&gt;
&lt;p&gt;There are pros and cons of both of them. For &lt;strong&gt;I/O-bound&lt;/strong&gt; operations (a characteristic of a typical web server) you get &lt;strong&gt;better performance&lt;/strong&gt; and &lt;strong&gt;higher number of concurrent requests&lt;/strong&gt; when you use a &lt;strong&gt;single-threaded event loop&lt;/strong&gt;. But the drawback is that you need to use exclusively asynchronous non-blocking I/O for all operations or otherwise you’ll block the event loop and lose performance. For that reason it’s easier to implement a multi-threaded server but you pay in performance.&lt;/p&gt;
&lt;p&gt;For &lt;strong&gt;CPU-bound&lt;/strong&gt; operations (less common for a usual web server, maybe more common for a computationally intensive API) it’s best to have &lt;strong&gt;one OS thread or process per core&lt;/strong&gt;. It’s easy to do with single-threaded event loops because you can run a cluster of a number of processes one per core. It’s hard to do with multi-threaded servers because if spawning threads is your only way to handle concurrent requests then you cannot really control how many threads you will have - as you don’t control the number of requests. Once you have more threads than the number of CPU cores then you loose performance for &lt;strong&gt;context switches&lt;/strong&gt; and you also use a lot of RAM.&lt;/p&gt;
&lt;p&gt;That is why a &lt;strong&gt;single-threaded nginx server&lt;/strong&gt; performs better than a multi-threaded Apache web server (and that is why nginx was created in the first place). Also &lt;strong&gt;Redis&lt;/strong&gt;, a database known for exceptionally high performance is &lt;strong&gt;single-threaded&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A real example I can give you is this: My first web server was Apache running on a Linux machine with 500MB of RAM. It forked a new process for every request (it actually had a pool so there was not much forking involved but it had to keep those processes alive to reuse them and kill them once in a while to avoid resource leakage).&lt;/p&gt;
&lt;p&gt;My OS used around 100MB of RAM. Every Apache process used 20MB of RAM. It meant that my server could only handle 20 concurrent requests and there was no way around it because I had no more RAM. The processes were mostly blocked on I/O so the CPU utilization was very low, every request above those 20 had to wait and if those 20 was e.g. long running downloads then my server was completely unresponsive.&lt;/p&gt;
&lt;p&gt;When nginx web server was introduced it used a single-threaded event loop and didn’t block for any request. It could handle much more concurrent requests, having no problem with the mythical c10k problem - nginx was basically created to solve the c10k problem (10,000 concurrent requests).&lt;/p&gt;
&lt;p&gt;Imagine how much RAM is wasted for 10,000 threads if you could even spawn that many and how much time is used for context switches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Memory usage of multi-threaded Apache vs single-threaded nginx:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pub-2a6758f3b2d64ef5bb71ba1601101d35.r2.dev/blogs/2025/01/3753c8d5681b143ae824ebc3830d959d.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Incidentally, this is the reason why Ryan Dahl used a non-blocking I/O and a single-threaded event loop in Node.js and he still uses the same idea in Deno, because that is the way to write high performance network servers (contrary to what you might read in other answers here).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that &lt;a href=&#34;https://nginx.org/en/docs/ngx_core_module.html#worker_processes&#34;&gt;nginx “core functionality” doc&lt;/a&gt; mentions that on most servers nginx defaults to multiple workers (which will be ran as threads) so it’s not always “single threaded.” &lt;a href=&#34;https://www.quora.com/profile/Alex-Sergeyev&#34;&gt;from a comment of this blog&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    </item>
    
    <item>
      <title>Hyper-Threading &amp; Physical Threads</title>
      <link>https://blog.jiyi27.com/posts/cs-basics/006-cpu-architecture/</link>
      <pubDate>Sun, 28 May 2023 15:47:18 +0000</pubDate>
      
      <guid>https://blog.jiyi27.com/posts/cs-basics/006-cpu-architecture/</guid>
      <description>&lt;h2 id=&#34;1-cpu-structure&#34;&gt;1. CPU structure&lt;/h2&gt;
&lt;p&gt;Single core CPU:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pub-2a6758f3b2d64ef5bb71ba1601101d35.r2.dev/blogs/2025/01/838e3b5018958914f986430b086270e2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The CPU core consists of three parts: ALU, CU and Memory (Register + Cache), The multiple cores CPU has more than one core (ALU, CU, Memory (Register + Cache)) to execute instructions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pub-2a6758f3b2d64ef5bb71ba1601101d35.r2.dev/blogs/2025/01/8556c886045ef0f880cf279f3724c1ef.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-hyper-threading&#34;&gt;2. Hyper-threading&lt;/h2&gt;
&lt;p&gt;A single physical core with hyper-threading or simultaneous multithreading technology appears as two logical cores to an operating system. The CPU is still a single CPU, so it’s a little bit of a cheat. This can speed things up somewhat — if one virtual CPU is stalled and waiting, the other virtual CPU can borrow its execution resources.&lt;/p&gt;</description>
      <content>&lt;h2 id=&#34;1-cpu-structure&#34;&gt;1. CPU structure&lt;/h2&gt;
&lt;p&gt;Single core CPU:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pub-2a6758f3b2d64ef5bb71ba1601101d35.r2.dev/blogs/2025/01/838e3b5018958914f986430b086270e2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The CPU core consists of three parts: ALU, CU and Memory (Register + Cache), The multiple cores CPU has more than one core (ALU, CU, Memory (Register + Cache)) to execute instructions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pub-2a6758f3b2d64ef5bb71ba1601101d35.r2.dev/blogs/2025/01/8556c886045ef0f880cf279f3724c1ef.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;2-hyper-threading&#34;&gt;2. Hyper-threading&lt;/h2&gt;
&lt;p&gt;A single physical core with hyper-threading or simultaneous multithreading technology appears as two logical cores to an operating system. The CPU is still a single CPU, so it’s a little bit of a cheat. This can speed things up somewhat — if one virtual CPU is stalled and waiting, the other virtual CPU can borrow its execution resources.&lt;/p&gt;
&lt;p&gt;Most processors can use a process called simultaneous multithreading or, if it’s an Intel processor, &lt;strong&gt;Hyper-threading&lt;/strong&gt; (the two terms mean the same thing) to &lt;strong&gt;split a core into virtual cores, which are called threads&lt;/strong&gt;. For example, AMD CPUs with four cores use simultaneous multithreading to provide eight threads, and most Intel CPUs with two cores use Hyper-threading to provide four threads.&lt;/p&gt;
&lt;p&gt;Some apps take better advantage of multiple threads than others. Lightly-threaded apps, like games, don&amp;rsquo;t benefit from a lot of cores, while most video editing and animation programs can run much faster with extra threads.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Strictly speaking, &lt;em&gt;only&lt;/em&gt; Intel processors have hyper-threading, however, the term is sometimes used colloquially to refer to any kind of simultaneous multithreading.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Windows Task Manager shows this fairly well. Here, for example, you can see that this system has one actual CPU (socket) and 8 cores. Simultaneous multithreading makes each core look like two CPUs to the operating system, so it shows 16 logical processors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pub-2a6758f3b2d64ef5bb71ba1601101d35.r2.dev/blogs/2025/01/75888124d5efc121b0ad287ac5b197aa.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;logical-core-vs-os-thread&#34;&gt;Logical core vs OS thread&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OS Thread（操作系统线程）&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程由操作系统内核管理，它可以调度线程在不同的 CPU 核心或逻辑处理器上运行。&lt;/li&gt;
&lt;li&gt;线程的调度和管理涉及 context switching, priority scheduling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hyper-threading（超线程）&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hyper-threading 是 Intel 提供的一种硬件级别的技术，它允许单个物理 CPU 核心模拟出两个逻辑处理器。&lt;/li&gt;
&lt;li&gt;当启用 Hyper-threading 时，操作系统会看到比实际物理核心数更多的处理器。例如，一个拥有 4 个物理核心的 CPU 可能会显示为 8 个逻辑处理器。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在操作系统管理线程的过程中，它会将多个 OS thread 分配给可用的 CPU 核心，包括通过 Hyper-threading 技术创建的逻辑处理器。这个分配过程考虑了多个因素，包括线程的优先级、CPU 亲和性（affinity）、以及核心的当前负载情况。因此，操作系统线程与 Hyper-threading 是协同工作的两个不同层面的概念：一个属于软件层面（操作系统管理），另一个属于硬件层面（CPU 架构）。&lt;/p&gt;
&lt;p&gt;参考:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.howtogeek.com/194756/cpu-basics-multiple-cpus-cores-and-hyper-threading-explained/&#34;&gt;CPU Basics: What Are Cores, Hyper-Threading, and Multiple CPUs?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.baeldung.com/cs/core-vs-cpu&#34;&gt;Differences Between Core and CPU | Baeldung on Computer Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tomshardware.com/news/cpu-core-definition,37658.html&#34;&gt;What Is a CPU Core? A Basic Definition | Tom&amp;rsquo;s Hardware&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Thread Stack and CPU Cores</title>
      <link>https://blog.jiyi27.com/posts/cs-basics/006-thread-process/</link>
      <pubDate>Sat, 27 May 2023 18:05:16 +0000</pubDate>
      
      <guid>https://blog.jiyi27.com/posts/cs-basics/006-thread-process/</guid>
      <description>&lt;h2 id=&#34;1-thread&#34;&gt;1. Thread&lt;/h2&gt;
&lt;p&gt;A thread is a segment or part of a process that executes some tasks of the process. A process can have multiple threads which can &lt;strong&gt;run concurrently&lt;/strong&gt; within the process. Each thread has its own thread stack but multiple threads of a process share a common heap area of that process.&lt;/p&gt;
&lt;h3 id=&#34;11-thread-stack&#34;&gt;1.1. Thread stack&lt;/h3&gt;
&lt;p&gt;Each thread has its own call stack, &amp;ldquo;call stack&amp;rdquo; and &amp;ldquo;thread stack&amp;rdquo; are the same thing. Calling it a &amp;ldquo;thread stack&amp;rdquo; just emphasizes that the call stack is specific to the thread.&lt;/p&gt;</description>
      <content>&lt;h2 id=&#34;1-thread&#34;&gt;1. Thread&lt;/h2&gt;
&lt;p&gt;A thread is a segment or part of a process that executes some tasks of the process. A process can have multiple threads which can &lt;strong&gt;run concurrently&lt;/strong&gt; within the process. Each thread has its own thread stack but multiple threads of a process share a common heap area of that process.&lt;/p&gt;
&lt;h3 id=&#34;11-thread-stack&#34;&gt;1.1. Thread stack&lt;/h3&gt;
&lt;p&gt;Each thread has its own call stack, &amp;ldquo;call stack&amp;rdquo; and &amp;ldquo;thread stack&amp;rdquo; are the same thing. Calling it a &amp;ldquo;thread stack&amp;rdquo; just emphasizes that the call stack is specific to the thread.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;stack&lt;/em&gt; is used to store variables used on the inside of a function (including the &lt;code&gt;main()&lt;/code&gt; function). It’s a LIFO, “&lt;strong&gt;L&lt;/strong&gt;ast-&lt;strong&gt;I&lt;/strong&gt;n,-&lt;strong&gt;F&lt;/strong&gt;irst-&lt;strong&gt;O&lt;/strong&gt;ut”, structure. Every time a function declares a new variable it is “pushed” onto the stack. Then when a function finishes running, all the variables associated with that function on the stack are deleted, and the memory they use is freed up. This leads to the “local” scope of function variables. The stack is a special region of memory, and automatically managed by the CPU – so you don’t have to allocate or deallocate memory. Stack memory is divided into successive frames where each time a function is called, it allocates itself a fresh &lt;strong&gt;stack frame&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Note that there is generally a limit on the size of the stack – which can vary with the operating system (for example OSX currently has a default stack size of 8MB). If a program tries to put too much information on the stack, &lt;strong&gt;stack overflow&lt;/strong&gt; will occur. Stack overflow happens when all the memory in the stack has been allocated, and further allocations begin overflowing into other sections of memory. Stack overflow also occurs in situations where recursion is incorrectly used.&lt;/p&gt;
&lt;h2 id=&#34;2-process&#34;&gt;2. Process&lt;/h2&gt;
&lt;p&gt;A program is a set of instructions. It is stored on a disk of a computer and hence it is &lt;strong&gt;Passive&lt;/strong&gt;. When the same program is loaded into the main memory and the OS assigns some heap memory to this program(application) is under execution is called a &lt;strong&gt;Process&lt;/strong&gt;. Hence a process is a program under execution. So we can say it is &lt;strong&gt;Active&lt;/strong&gt;. A process can create child processes by using the &lt;strong&gt;fork&lt;/strong&gt; system calls.&lt;/p&gt;
&lt;h2 id=&#34;3-relationship-between-a-thread-and-a-cpu-core&#34;&gt;3. Relationship between a thread and a CPU core&lt;/h2&gt;
&lt;p&gt;A CPU core is a physical processing unit in a computer’s central processing unit (CPU) that can execute instructions independently. A thread, on the other hand, is a unit of execution within a process, which represents a sequence of instructions that can be executed independently by a CPU.&lt;/p&gt;
&lt;p&gt;In general, the number of threads that can be executed simultaneously on a CPU is limited by the number of cores available in the CPU. Each core can execute one thread at a time, so having multiple cores allows for multiple threads to be executed in parallel, potentially leading to improved performance.&lt;/p&gt;
&lt;p&gt;However, the relationship between threads and CPU cores is more complex than just one-to-one mapping.&lt;/p&gt;
&lt;p&gt;In modern computer systems, threads can be scheduled dynamically on different cores by the operating system, and a single core can switch between multiple threads in order to maximize the utilization of available resources and CPU cores.&lt;/p&gt;
&lt;p&gt;Additionally, some systems may also use techniques such as &lt;strong&gt;hyper-threading&lt;/strong&gt;, where a single physical core is treated as multiple virtual cores, potentially allowing for even more threads to be executed simultaneously.&lt;/p&gt;
&lt;p&gt;Note that **simultaneous not equals to parallel. **&lt;/p&gt;
&lt;p&gt;From this can also see the importance of those basic undergraduate courses, the principles of computer composition of a lot of content, including the CPU architecture, registers, buses, memory structure, how the CPU reads commands from the registers, which provides the basis for future operating system courses. For example, now we are learning about threads, processes, which are all part of the operating system curriculum, and hyper-threading, if you don&amp;rsquo;t know how the CPU handles instructions and how it waits for the bus to send data, how can you understand the interrupt system very well? Golang is very popular now, it is very good at concurrency, Goroutine is very lightweight, but why is goroutine lightweight? You&amp;rsquo;re probably going to get asked that in an interview, right? These are context switches, and you can&amp;rsquo;t understand why goroutines are so powerful without learning the above, but that&amp;rsquo;s just one example. This is just one example. Just one concurrency problem, and that&amp;rsquo;s a lot of knowledge and lessons. The rest of the course such as the network, compilation principles, are very important, may not have an immediate effect, but they will be the future to support you the most solid foundation of the building.&lt;/p&gt;
&lt;p&gt;Related article:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://davidzhu.xyz/post/cs-basics/008-context-switching/&#34;&gt;Context Switching - David&amp;rsquo;s Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://davidzhu.xyz/post/cs-basics/006-cpu-architecture/&#34;&gt;Hyper-Threading &amp;amp; Physical Threads - David&amp;rsquo;s Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/javarevisited/process-and-thread-context-switching-do-you-know-the-difference-updated-8fd93877dff6&#34;&gt;Process and Thread Context Switching, Do You Know the Difference? &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/31145052/difference-between-call-stack-and-thread-stack&#34;&gt;java - Difference between &amp;ldquo;call stack&amp;rdquo; and &amp;ldquo;thread stack&amp;rdquo; - Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://craftofcoding.wordpress.com/2015/12/07/memory-in-c-the-stack-the-heap-and-static/&#34;&gt;Memory in C – the stack, the heap, and static – The Craft of Coding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Context Switching</title>
      <link>https://blog.jiyi27.com/posts/cs-basics/006-context-switching/</link>
      <pubDate>Sat, 27 May 2023 16:29:15 +0000</pubDate>
      
      <guid>https://blog.jiyi27.com/posts/cs-basics/006-context-switching/</guid>
      <description>&lt;h2 id=&#34;1-context-switch&#34;&gt;1. Context switch&lt;/h2&gt;
&lt;p&gt;In a CPU, the term &amp;ldquo;context&amp;rdquo; refers to the data in the registers and program counter (PC) at a specific moment in time. A register holds the current CPU instruction. A program counter, also known as an instruction address register, is a small amount of fast memory that holds the address of the instruction to be executed immediately after the current one.&lt;/p&gt;
&lt;p&gt;In computing, a context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point, and then restoring a different, previously saved, state. Two steps, the first step is to &lt;strong&gt;store the state of the thread and then restore the state of another&lt;/strong&gt;.&lt;/p&gt;</description>
      <content>&lt;h2 id=&#34;1-context-switch&#34;&gt;1. Context switch&lt;/h2&gt;
&lt;p&gt;In a CPU, the term &amp;ldquo;context&amp;rdquo; refers to the data in the registers and program counter (PC) at a specific moment in time. A register holds the current CPU instruction. A program counter, also known as an instruction address register, is a small amount of fast memory that holds the address of the instruction to be executed immediately after the current one.&lt;/p&gt;
&lt;p&gt;In computing, a context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point, and then restoring a different, previously saved, state. Two steps, the first step is to &lt;strong&gt;store the state of the thread and then restore the state of another&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;2-two-data-structure-pcb--tcb&#34;&gt;2. Two data structure: PCB &amp;amp; TCB&lt;/h2&gt;
&lt;p&gt;The &amp;lsquo;state&amp;rsquo; mentioned above is thread or process related information, stored in PCB (Process) and TCB (Thread) respectively.&lt;/p&gt;
&lt;h3 id=&#34;21-process-control-block-pcb&#34;&gt;2.1 Process control block (PCB)&lt;/h3&gt;
&lt;p&gt;A process control block (PCB) contains information about the process, i.e. registers, PID, priority, etc. The process table is an array of PCBs, that means logically contains a PCB for all of the current processes in the system.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Process State – new, ready, running, waiting, dead;&lt;/li&gt;
&lt;li&gt;Process Number (PID) – unique identification number for each process (also known as Process ID);&lt;/li&gt;
&lt;li&gt;Program Counter (PC) – a pointer to the address of the next instruction to be executed for this process;&lt;/li&gt;
&lt;li&gt;CPU Registers – register set where process needs to be stored for execution for running state;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22-thread-control-block-tcb&#34;&gt;2.2 &lt;strong&gt;Thread control block&lt;/strong&gt; (&lt;strong&gt;TCB&lt;/strong&gt;)&lt;/h3&gt;
&lt;p&gt;An example of information contained within a TCB is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Thread Identifier: Unique id (tid) is assigned to every new thread&lt;/li&gt;
&lt;li&gt;Stack pointer: Points to thread&amp;rsquo;s stack in the process&lt;/li&gt;
&lt;li&gt;Program counter (PC): Points to the current program instruction of the thread&lt;/li&gt;
&lt;li&gt;State of the thread (running, ready, waiting, start, done)&lt;/li&gt;
&lt;li&gt;Thread&amp;rsquo;s register values&lt;/li&gt;
&lt;li&gt;Pointer to the Process control block (PCB) of the process that the thread lives on&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-cost-of-context-switch&#34;&gt;3. Cost of context switch&lt;/h2&gt;
&lt;p&gt;Switching from one process to another requires a certain amount of time for doing the administration – saving and loading registers and memory maps, updating various tables and lists, etc.&lt;/p&gt;
&lt;p&gt;For example, in the Linux kernel, context switching involves &lt;em&gt;&lt;strong&gt;loading the corresponding process control block (PCB)&lt;/strong&gt;&lt;/em&gt; stored in the PCB table in the kernel stack to retrieve information about the state of the new process. &lt;em&gt;&lt;strong&gt;CPU state information&lt;/strong&gt;&lt;/em&gt; including the registers, stack pointer, and program counter as well as memory management information like segmentation tables and page tables (unless the old process shares the memory with the new) are loaded from the PCB for the new process. To avoid incorrect address translation in the case of the previous and current processes using different memory, &lt;em&gt;&lt;strong&gt;the translation lookaside buffer (TLB)&lt;/strong&gt;&lt;/em&gt; must be flushed. This negatively affects performance because every memory reference to the TLB will be a miss because it is empty after most context switches.&lt;/p&gt;
&lt;p&gt;Furthermore, analogous context switching happens between &lt;a href=&#34;https://en.wikipedia.org/wiki/User_thread&#34;&gt;user threads&lt;/a&gt;, notably &lt;a href=&#34;https://en.wikipedia.org/wiki/Green_thread&#34;&gt;green threads&lt;/a&gt;, and is often very lightweight, saving and restoring minimal context. In extreme cases, such as switching between goroutines in &lt;a href=&#34;https://en.wikipedia.org/wiki/Go_%28programming_language%29&#34;&gt;Go&lt;/a&gt;, a context switch is equivalent to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Coroutine&#34;&gt;coroutine&lt;/a&gt; yield, which is only marginally more expensive than a &lt;a href=&#34;https://en.wikipedia.org/wiki/Subroutine&#34;&gt;subroutine&lt;/a&gt; call.&lt;/p&gt;
&lt;h2 id=&#34;4-when-context-switch-happens&#34;&gt;4. When context switch happens&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;System calls&lt;/strong&gt;: when a process makes any system calls, the OS switches the mode of the kernel and saves that process in context, and executes the system call.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interrupt handling:&lt;/strong&gt; Modern architectures are &lt;a href=&#34;https://en.wikipedia.org/wiki/Interrupt&#34;&gt;interrupt&lt;/a&gt; driven. This means that if the CPU requests data from a disk, for example, it does not need to &lt;a href=&#34;https://en.wikipedia.org/wiki/Busy-wait&#34;&gt;busy-wait&lt;/a&gt; until the read is over; it can issue the request (to the I/O device) and continue with some other task. When the read is over, the CPU can be &lt;em&gt;interrupted&lt;/em&gt; (by a hardware in this case, which sends interrupt request to &lt;a href=&#34;https://en.wikipedia.org/wiki/Programmable_interrupt_controller&#34;&gt;PIC&lt;/a&gt;) and presented with the read. For interrupts, a program called an &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Interrupt_handler&#34;&gt;interrupt handler&lt;/a&gt;&lt;/em&gt; is installed, and it is the interrupt handler that handles the interrupt from the disk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;User and Kernel Mode switching&lt;/strong&gt;: this trigger is used when the OS needed to switch between the user mode and kernel mode.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5-performance&#34;&gt;5. Performance&lt;/h2&gt;
&lt;p&gt;Context switching itself has a cost in performance, due to running the task scheduler, TLB flushes, and indirectly due to sharing the CPU cache between multiple tasks. &lt;strong&gt;Switching between threads of a single process can be faster than between two separate processes, because threads share the same virtual memory maps, so a TLB flush is not necessary&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;program counter (PC): processor register, stores the address of next instruction to be executed.&lt;/li&gt;
&lt;li&gt;context switch: store state, restore state&lt;/li&gt;
&lt;li&gt;causes of context siwtch
&lt;ul&gt;
&lt;li&gt;system call&lt;/li&gt;
&lt;li&gt;interrupt handling: CPU requests data from a disk&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Context_switch&#34;&gt;Context switch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Process_control_block&#34;&gt;Process control block&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Thread_control_block&#34;&gt;Thread control block&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Program_counter&#34;&gt;Program counter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.geeksforgeeks.org/context-switch-in-operating-system/&#34;&gt;Context Switch in Operating System - GeeksforGeeks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html&#34;&gt;Scheduling In Go : Part I - OS Scheduler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
